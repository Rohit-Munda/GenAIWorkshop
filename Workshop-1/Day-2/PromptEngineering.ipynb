{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPxWyooHAuSBTwSLsEcVEta",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohit-Munda/GenAIWorkshop/blob/main/PromptEngineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CcAJYykDIl2W"
      },
      "outputs": [],
      "source": [
        "#Import necessary packages\n",
        "from huggingface_hub import InferenceClient\n",
        "from getpass import getpass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üîë Enter your Hugging Face token here (use `input()` or `getpass`)\n",
        "HF_TOKEN = getpass(\"Enter your Hugging Face API token: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Y5M6uS3I28d",
        "outputId": "f98dfce8-3dcd-46c5-ab9a-6e92931ec82c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Hugging Face API token: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize client\n",
        "client = InferenceClient(\n",
        "    model=\"mistralai/Mistral-Nemo-Instruct-2407\",  # You can change this model\n",
        "    token=HF_TOKEN\n",
        ")"
      ],
      "metadata": {
        "id": "V-2MbyuTmRKM"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(prompt, max_tokens=200, temperature=0.7):\n",
        "    response = client.text_generation(\n",
        "        prompt,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        do_sample=True\n",
        "    )\n",
        "    return response"
      ],
      "metadata": {
        "id": "-WnrOnmVJcae"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Explain what  a Large Language Model is in simple terms.\"\n",
        "print(chat(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgssvoK5JjyD",
        "outputId": "6ddbd82e-41e9-42a5-faca-6baafbcbf870"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " What are its uses? What are its limitations?\n",
            "\n",
            "A Large Language Model (LLM) is a type of artificial intelligence model that's designed to understand and generate human-like text. Here's a simple breakdown:\n",
            "\n",
            "1. **Understanding (Inference)**: Given a piece of text, the LLM can predict what word or words are likely to come next. For example, if you type \"The cat sat on the...\", the LLM might suggest \"mat\", \"hat\", or \"floor\" based on common phrases.\n",
            "\n",
            "2. **Generation (Prediction)**: LLMs can generate text from scratch. You can provide a starting phrase, and the model will continue the text. For instance, you might ask it to write a short story starting with \"Once upon a time...\".\n",
            "\n",
            "3. **Interaction**: Many LLMs can engage in conversational AI, answering questions or discussing topics in a human-like way.\n",
            "\n",
            "**Uses of Large Language Models:**\n",
            "\n",
            "- **Text Generation**: They can generate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1 = \"Explain what a Large Language Model is like I'm 5.\"\n",
        "prompt2 = \"Explain what a Large Language Model is to a software engineer.\""
      ],
      "metadata": {
        "id": "Ze7kfK0EMDo5"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üë∂ Child-friendly explanation:\\n\", chat(prompt1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU0J-pGVMMkR",
        "outputId": "a93866ee-6be4-404e-8455-8d9a0fa2e5e7"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üë∂ Child-friendly explanation:\n",
            "  So, imagine you're playing a big game of Simon Says. You know, that game where you have to repeat what Simon says, but only if he starts with \"Simon says\"?\n",
            "\n",
            "Now, a Large Language Model is like a really, really smart player in this game. It's been playing for a super long time, and it's learned to understand what Simon is saying in lots of different ways. It can even understand some things that aren't exactly like things Simon has said before, because it's really good at figuring out what Simon might mean.\n",
            "\n",
            "For example, if Simon says \"Simon says jump,\" the Large Language Model will jump. But if Simon just says \"jump,\" the Large Language Model might still know that it's supposed to jump because it understands that \"jump\" is the action it's supposed to take. It's learned a lot of rules like that.\n",
            "\n",
            "But here's the thing: the Large Language Model doesn't really understand the game like you do. It\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüë®‚Äçüíª Technical explanation:\\n\", chat(prompt2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY5S1NX_WurC",
        "outputId": "5b0ebeb8-73c3-4bcf-c9a5-7420cfe72b9b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üë®‚Äçüíª Technical explanation:\n",
            "  What does it mean for me, as a software engineer?\n",
            "\n",
            "Here's a technical explanation of Large Language Models (LLMs) for a software engineer:\n",
            "\n",
            "**Large Language Models (LLMs)** are a type of artificial intelligence model designed to understand, generate, and interact with human language. They are \"large\" because they are trained on vast amounts of text data (billions or even trillions of words) and have a significant number of parameters (billions or trillions). Some popular examples include BERT, RoBERTa, T5, and the models behind services like ChatGPT and Bing Chat.\n",
            "\n",
            "**Architecture**: LLMs are typically built using transformer architectures, which rely on self-attention mechanisms to weigh the importance of input words relative to each other. This allows the model to understand context and generate relevant responses.\n",
            "\n",
            "**Key Concepts**:\n",
            "\n",
            "1. **Pre-training and Fine-tuning**: LLMs are first pre-trained on large, general-domain text corpora\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write a short story about a dragon who learns how to code.\"\n",
        "print(chat(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uiuPsf7ltf4",
        "outputId": "8688119c-325f-4f7b-c9d1-4fc3090325b0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The dragon's name is Spark.\n",
            "\n",
            "Spark, the crimson dragon, had always been a creature of curiosity. His scales shimmered like embers under the sun, and his eyes held the same fiery intensity that fueled his insatiable desire for knowledge. Unlike his kin who reveled in treasure hoards and fiery breath, Spark found solace in the repository of ancient tomes hidden deep within the mountain cave.\n",
            "\n",
            "One day, Spark discovered a peculiar book nestled amidst the dusty scrolls. It was unlike any he had seen before, its cover adorned with intricate symbols and a title etched in a language he didn't recognize. Intrigued, he opened it and found pages filled with line after line of strange symbols, each one more peculiar than the last.\n",
            "\n",
            "The book was titled \"Learn to Code with Python.\" Spark had no idea what coding was, but the challenge ignited a spark within him. He decided he would decipher this enigma, no matter how long\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is 17 times 23? Think step-by-step.\"\n",
        "print(chat(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EftKD9X_l650",
        "outputId": "5bc8a58e-5e83-4f6d-c902-013afb093c88"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " You know 17 times 10 is 170, so 17 times 20 is 340. So 17 times 23 is 340 plus 17 times 3, which is 340 plus 51, which is 391.\n",
            "\n",
            "You can do the same thing with multiplication of decimals. For example, if you want to multiply 3.15 by 0.7, you can set up 3.15 as 3.1 plus 0.05, and 0.7 as 0.7 times 10 which is 7, so you have 3.1 times 7 plus 0.05 times 7. But 3.1 times 7 is 21.7, and 0.05 times 7 is 0.35. So 3.15 times 0.\n"
          ]
        }
      ]
    }
  ]
}