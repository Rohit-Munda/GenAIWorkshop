{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNNzl4A/q+Blz5Go/bvcFHI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohit-Munda/GenAIWorkshop/blob/main/Workshop-1/Day-2/PromptEngineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prompt Engineering"
      ],
      "metadata": {
        "id": "s2bvtxOg7RD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Load Required Python Libraries"
      ],
      "metadata": {
        "id": "4D-9MRSX7i-X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CcAJYykDIl2W"
      },
      "outputs": [],
      "source": [
        "#Import necessary packages\n",
        "from huggingface_hub import InferenceClient\n",
        "from getpass import getpass"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2:  Provide Hugging Face Authentication Token"
      ],
      "metadata": {
        "id": "akC5oE7Z72K5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîë Enter your Hugging Face token here (use `input()` or `getpass`)\n",
        "HF_TOKEN = getpass(\"Enter your Hugging Face API token: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Y5M6uS3I28d",
        "outputId": "f135fff1-143e-4392-bad8-2a27a76a03a0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Hugging Face API token: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Initialize Model"
      ],
      "metadata": {
        "id": "UOZCUhUc8E5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize client\n",
        "client = InferenceClient(\n",
        "    model=\"mistralai/Mistral-Nemo-Instruct-2407\",  # You can change this model\n",
        "    token=HF_TOKEN\n",
        ")"
      ],
      "metadata": {
        "id": "V-2MbyuTmRKM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Define chat function to call model with prompt"
      ],
      "metadata": {
        "id": "oRovZf4T8M8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(prompt, max_tokens=200, temperature=0.7):\n",
        "    response = client.text_generation(\n",
        "        prompt,\n",
        "        max_new_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        do_sample=True\n",
        "    )\n",
        "    return response"
      ],
      "metadata": {
        "id": "-WnrOnmVJcae"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Execute function with different prompts"
      ],
      "metadata": {
        "id": "YFMdm-I38Tyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Explain what  a Large Language Model is in simple terms.\"\n",
        "print(chat(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgssvoK5JjyD",
        "outputId": "ffc2cccc-4158-4554-ecb5-0d7d509bc78d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " For example, \"A Large Language Model is like a book that has read a lot and can understand and generate text based on that understanding.\" Please also provide a brief history of Large Language Models.\n",
            "\n",
            "Sure, I'd be happy to explain Large Language Models (LLMs) in simple terms and provide a brief history.\n",
            "\n",
            "**What is a Large Language Model?**\n",
            "\n",
            "Imagine you have a very smart friend who has read almost every book, article, and conversation in existence. This friend can understand the meaning of words, phrases, and sentences incredibly well. They can also generate new, human-like text based on what they've understood. Now, replace this friend with a computer algorithm, and you have a Large Language Model.\n",
            "\n",
            "In more technical terms, a Large Language Model is a type of artificial intelligence model that's trained on a vast amount of text data. This training allows the model to understand and generate human-like text. It can do this because it's learned patterns, grammar rules, and even some context from\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt1 = \"Explain what a Large Language Model is like I'm 5.\"\n",
        "prompt2 = \"Explain what a Large Language Model is to a software engineer.\""
      ],
      "metadata": {
        "id": "Ze7kfK0EMDo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"üë∂ Child-friendly explanation:\\n\", chat(prompt1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rU0J-pGVMMkR",
        "outputId": "a93866ee-6be4-404e-8455-8d9a0fa2e5e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üë∂ Child-friendly explanation:\n",
            "  So, imagine you're playing a big game of Simon Says. You know, that game where you have to repeat what Simon says, but only if he starts with \"Simon says\"?\n",
            "\n",
            "Now, a Large Language Model is like a really, really smart player in this game. It's been playing for a super long time, and it's learned to understand what Simon is saying in lots of different ways. It can even understand some things that aren't exactly like things Simon has said before, because it's really good at figuring out what Simon might mean.\n",
            "\n",
            "For example, if Simon says \"Simon says jump,\" the Large Language Model will jump. But if Simon just says \"jump,\" the Large Language Model might still know that it's supposed to jump because it understands that \"jump\" is the action it's supposed to take. It's learned a lot of rules like that.\n",
            "\n",
            "But here's the thing: the Large Language Model doesn't really understand the game like you do. It\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüë®‚Äçüíª Technical explanation:\\n\", chat(prompt2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RY5S1NX_WurC",
        "outputId": "5b0ebeb8-73c3-4bcf-c9a5-7420cfe72b9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üë®‚Äçüíª Technical explanation:\n",
            "  What does it mean for me, as a software engineer?\n",
            "\n",
            "Here's a technical explanation of Large Language Models (LLMs) for a software engineer:\n",
            "\n",
            "**Large Language Models (LLMs)** are a type of artificial intelligence model designed to understand, generate, and interact with human language. They are \"large\" because they are trained on vast amounts of text data (billions or even trillions of words) and have a significant number of parameters (billions or trillions). Some popular examples include BERT, RoBERTa, T5, and the models behind services like ChatGPT and Bing Chat.\n",
            "\n",
            "**Architecture**: LLMs are typically built using transformer architectures, which rely on self-attention mechanisms to weigh the importance of input words relative to each other. This allows the model to understand context and generate relevant responses.\n",
            "\n",
            "**Key Concepts**:\n",
            "\n",
            "1. **Pre-training and Fine-tuning**: LLMs are first pre-trained on large, general-domain text corpora\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Write a short story about a dragon who learns how to code.\"\n",
        "print(chat(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uiuPsf7ltf4",
        "outputId": "8688119c-325f-4f7b-c9d1-4fc3090325b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The dragon's name is Spark.\n",
            "\n",
            "Spark, the crimson dragon, had always been a creature of curiosity. His scales shimmered like embers under the sun, and his eyes held the same fiery intensity that fueled his insatiable desire for knowledge. Unlike his kin who reveled in treasure hoards and fiery breath, Spark found solace in the repository of ancient tomes hidden deep within the mountain cave.\n",
            "\n",
            "One day, Spark discovered a peculiar book nestled amidst the dusty scrolls. It was unlike any he had seen before, its cover adorned with intricate symbols and a title etched in a language he didn't recognize. Intrigued, he opened it and found pages filled with line after line of strange symbols, each one more peculiar than the last.\n",
            "\n",
            "The book was titled \"Learn to Code with Python.\" Spark had no idea what coding was, but the challenge ignited a spark within him. He decided he would decipher this enigma, no matter how long\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is 17 times 23? Think step-by-step.\"\n",
        "print(chat(prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EftKD9X_l650",
        "outputId": "5bc8a58e-5e83-4f6d-c902-013afb093c88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " You know 17 times 10 is 170, so 17 times 20 is 340. So 17 times 23 is 340 plus 17 times 3, which is 340 plus 51, which is 391.\n",
            "\n",
            "You can do the same thing with multiplication of decimals. For example, if you want to multiply 3.15 by 0.7, you can set up 3.15 as 3.1 plus 0.05, and 0.7 as 0.7 times 10 which is 7, so you have 3.1 times 7 plus 0.05 times 7. But 3.1 times 7 is 21.7, and 0.05 times 7 is 0.35. So 3.15 times 0.\n"
          ]
        }
      ]
    }
  ]
}