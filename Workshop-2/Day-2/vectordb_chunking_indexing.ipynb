{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMglw43A2jiPiMGkoSHJ3Wh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohit-Munda/GenAIWorkshop/blob/main/vectordb_chunking_indexing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧠 Vector Database Workshop: Introduction to ChromaDB & Indexing"
      ],
      "metadata": {
        "id": "ng79BT2xRkeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome! In this hands-on session, we will:\n",
        "\n",
        "\n",
        "*   Setup a vector database\n",
        "*   Chunk documents and store in vector database\n",
        "*   Similarity search from vector store"
      ],
      "metadata": {
        "id": "lcnxu_6fRseo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain_community chromadb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01gjiUOkRmPy",
        "outputId": "9d0aee38-0b6a-48bc-b0cd-c0733751f667"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m90.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.4/188.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.0/119.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📄 Step 1: Create and Load a Sample Document"
      ],
      "metadata": {
        "id": "JfYrxFtdWBfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "# Create a sample text file\n",
        "sample_text = (\n",
        "    \"Artificial Intelligence (AI) is transforming industries across the globe. \"\n",
        "    \"From healthcare to finance, AI applications are driving innovation. \"\n",
        "    \"Large Language Models (LLMs) are at the core of this revolution, enabling machines to understand and generate human-like language. \"\n",
        "    \"In this session, we explore how to prepare documents for LLMs using LangChain.\"\n",
        ")\n",
        "\n",
        "with open(\"sample_doc.txt\", \"w\") as f:\n",
        "    f.write(sample_text)\n",
        "\n",
        "# Load the document\n",
        "loader = TextLoader(\"sample_doc.txt\")\n",
        "documents = loader.load()\n",
        "\n",
        "print(\"Loaded document:\")\n",
        "print(documents[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ukMux3RV6CV",
        "outputId": "51ae4eba-49b5-4990-ef26-4d40a97e942c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded document:\n",
            "Artificial Intelligence (AI) is transforming industries across the globe. From healthcare to finance, AI applications are driving innovation. Large Language Models (LLMs) are at the core of this revolution, enabling machines to understand and generate human-like language. In this session, we explore how to prepare documents for LLMs using LangChain.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔁 Step 2: Recursive Splitting with `RecursiveCharacterTextSplitter`"
      ],
      "metadata": {
        "id": "BQC0okDlWZ7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "recursive_splitter = RecursiveCharacterTextSplitter(chunk_size=50, chunk_overlap=15)\n",
        "recursive_chunks = recursive_splitter.split_documents(documents)\n",
        "\n",
        "print(f\"Number of chunks: {len(recursive_chunks)}\")\n",
        "for i, chunk in enumerate(recursive_chunks):\n",
        "    print(f\"Chunk {i+1}: {chunk.page_content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyIbBpohWYrb",
        "outputId": "610f3b40-2e89-46bc-b60f-5d9798501f64"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks: 11\n",
            "Chunk 1: Artificial Intelligence (AI) is transforming\n",
            "Chunk 2: transforming industries across the globe. From\n",
            "Chunk 3: globe. From healthcare to finance, AI\n",
            "Chunk 4: to finance, AI applications are driving\n",
            "Chunk 5: are driving innovation. Large Language Models\n",
            "Chunk 6: Models (LLMs) are at the core of this revolution,\n",
            "Chunk 7: revolution, enabling machines to understand and\n",
            "Chunk 8: understand and generate human-like language. In\n",
            "Chunk 9: language. In this session, we explore how to\n",
            "Chunk 10: explore how to prepare documents for LLMs using\n",
            "Chunk 11: for LLMs using LangChain.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recursive_chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6uxEMpFWw4Q",
        "outputId": "e95b2d0f-75c8-4f9e-c049-b29b848027e4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'sample_doc.txt'}, page_content='Artificial Intelligence (AI) is transforming'),\n",
              " Document(metadata={'source': 'sample_doc.txt'}, page_content='transforming industries across the globe. From'),\n",
              " Document(metadata={'source': 'sample_doc.txt'}, page_content='globe. From healthcare to finance, AI'),\n",
              " Document(metadata={'source': 'sample_doc.txt'}, page_content='to finance, AI applications are driving'),\n",
              " Document(metadata={'source': 'sample_doc.txt'}, page_content='are driving innovation. Large Language Models'),\n",
              " Document(metadata={'source': 'sample_doc.txt'}, page_content='Models (LLMs) are at the core of this revolution,'),\n",
              " Document(metadata={'source': 'sample_doc.txt'}, page_content='revolution, enabling machines to understand and'),\n",
              " Document(metadata={'source': 'sample_doc.txt'}, page_content='understand and generate human-like language. In'),\n",
              " Document(metadata={'source': 'sample_doc.txt'}, page_content='language. In this session, we explore how to'),\n",
              " Document(metadata={'source': 'sample_doc.txt'}, page_content='explore how to prepare documents for LLMs using'),\n",
              " Document(metadata={'source': 'sample_doc.txt'}, page_content='for LLMs using LangChain.')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📥 Step 3: Initialize Chroma database and create collection"
      ],
      "metadata": {
        "id": "9gV0nPyAXGOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "# Disable telemetry\n",
        "chroma_client = chromadb.Client(Settings(anonymized_telemetry=False))\n",
        "\n",
        "# Create a new collection for your text chunks\n",
        "collection = chroma_client.create_collection(name=\"text_chunks_demo\")"
      ],
      "metadata": {
        "id": "6WdGPyEOXN2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📄 Step 4: Generate embeddings"
      ],
      "metadata": {
        "id": "RnSTQj71X4fC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load embedding model\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "#Create list of text from recursive_chunks\n",
        "chunk_texts = [chunk.page_content for chunk in recursive_chunks]\n",
        "\n",
        "# Generate embeddings\n",
        "embeddings = embedding_model.encode(chunk_texts).tolist()\n",
        "print(f\"Created {len(embeddings)} embeddings.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qohTWOBwXVc2",
        "outputId": "5c48eed5-8692-4bcf-e1c7-ae921b738c08"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 11 embeddings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📝 Step 5: Add Chunks and Embeddings to Chroma"
      ],
      "metadata": {
        "id": "3rMIS2aaaC4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add each chunk with its embedding to the Chroma collection\n",
        "# We'll use the chunk index as the ID\n",
        "ids = [f\"chunk_{i}\" for i in range(len(chunk_texts))]\n",
        "\n",
        "collection.add(\n",
        "    documents=chunk_texts,\n",
        "    embeddings=embeddings,\n",
        "    ids=ids\n",
        ")\n",
        "\n",
        "print(\"Chunks and their embeddings have been added to Chroma!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82qN1F3wZ7rr",
        "outputId": "8087914e-d780-43ee-963f-f174c69cd58d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunks and their embeddings have been added to Chroma!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🔍 Step 8: Similarity Search in Chroma"
      ],
      "metadata": {
        "id": "kzpNMSAraZny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a query\n",
        "query = \"What are Large Language Models?\"\n",
        "\n",
        "# Embed the query\n",
        "query_embedding = embedding_model.encode([query]).tolist()\n",
        "\n",
        "# Search in Chroma\n",
        "results = collection.query(\n",
        "    query_embeddings=query_embedding,\n",
        "    n_results=3  # Show top 3 most similar chunks\n",
        ")\n",
        "\n",
        "print(\"Most similar chunks to your query:\")\n",
        "for doc, score in zip(results['documents'][0], results['distances'][0]):\n",
        "    print(f\"Score: {score:.4f} | Chunk: {doc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkK1kljzaSe_",
        "outputId": "f27515af-e537-4eeb-ebc6-3f77ef73e8d3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar chunks to your query:\n",
            "Score: 0.7126 | Chunk: are driving innovation. Large Language Models\n",
            "Score: 1.0036 | Chunk: understand and generate human-like language. In\n",
            "Score: 1.1861 | Chunk: language. In this session, we explore how to\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Summary (Add to your summary section)\n",
        "You learned how to split text, create embeddings, store them in a vector database (Chroma), and perform similarity search.\n",
        "\n",
        "This is the foundation for search and retrieval in modern AI applications!"
      ],
      "metadata": {
        "id": "o4ne5_HXawka"
      }
    }
  ]
}